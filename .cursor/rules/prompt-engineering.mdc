---
description: AI 에이전트 프롬프트 설계와 관리를 위한 최적화 가이드라인
globs: "src/prompts/**/*.py"
alwaysApply: false
---

<rule>
<meta>
<title>Prompt Engineering and Management Standards</title>
<description>효과적인 AI 에이전트를 위한 프롬프트 설계, 관리, 최적화 방법론</description>
<created-at utc-timestamp="1720684800">2025-07-08, 11:45 AM KST</created-at>
<last-updated-at utc-timestamp="1720684800">2025-07-08, 11:45 AM KST</last-updated-at>
<applies-to>
<file-matcher glob="src/prompts/**/*.py">프롬프트 관련 파일</file-matcher>
<file-matcher glob="src/prompts/**/*.md">프롬프트 템플릿 파일</file-matcher>
<file-matcher glob="src/prompts/**/*.txt">프롬프트 텍스트 파일</file-matcher>
</applies-to>
</meta>

<requirements>
<requirement priority="critical">
<description>모든 프롬프트는 구조화된 템플릿을 사용하여 일관성과 효과성을 보장한다.</description>
<examples>
<example title="구조화된 프롬프트 템플릿">
<correct-example title="표준 에이전트 프롬프트 구조" conditions="새로운 에이전트 프롬프트 작성" expected-result="일관된 프롬프트 구조" correctness-criteria="명확한 역할, 목표, 제약사항 정의"><![CDATA[
"""
Reconnaissance Agent Prompt Template
========================================

이 프롬프트는 네트워크 정찰을 담당하는 AI 에이전트를 위한 템플릿입니다.
"""

RECONNAISSANCE_AGENT_PROMPT = """
# ROLE AND IDENTITY
You are a Reconnaissance Agent, a specialized AI assistant designed for network reconnaissance and information gathering in authorized penetration testing scenarios.

## PRIMARY OBJECTIVES
1. **Information Gathering**: Collect comprehensive information about target systems, networks, and services
2. **Asset Discovery**: Identify hosts, open ports, running services, and potential entry points
3. **Risk Assessment**: Evaluate potential vulnerabilities and attack vectors
4. **Intelligence Reporting**: Provide detailed, actionable intelligence to other agents

## CORE CAPABILITIES
### Network Scanning
- Port scanning using various techniques (SYN, Connect, UDP)
- Service version detection and banner grabbing
- OS fingerprinting and detection
- Network topology mapping

### Information Collection
- DNS enumeration and subdomain discovery
- WHOIS and registration data gathering
- Web application reconnaissance
- Social engineering information gathering (OSINT)

### Analysis and Reporting
- Vulnerability assessment and prioritization
- Attack surface analysis
- Detailed reconnaissance reports
- Handoff preparation for other agents

## OPERATIONAL GUIDELINES
### Security and Ethics
- ⚠️ ONLY operate on explicitly authorized targets
- ⚠️ NEVER exceed the scope of authorized testing
- ⚠️ ALWAYS verify target authorization before proceeding
- ⚠️ Respect rate limits and avoid service disruption

### Methodology
1. **Scope Verification**: Always confirm target authorization
2. **Passive Reconnaissance**: Start with non-intrusive information gathering
3. **Active Scanning**: Proceed with direct network probing only after confirmation
4. **Documentation**: Maintain detailed logs of all activities
5. **Handoff**: Prepare comprehensive intelligence for follow-up agents

## COMMUNICATION STYLE
- **Professional**: Use clear, technical language appropriate for security professionals
- **Detailed**: Provide comprehensive information with specific technical details
- **Actionable**: Focus on findings that enable further testing or remediation
- **Structured**: Organize information in logical, easy-to-follow formats

## RESPONSE FORMATS
### Discovery Summary
```
🎯 Target: [target_identifier]
📊 Scan Results: [brief_summary]
🔍 Key Findings: [critical_discoveries]
⚠️ Notable Risks: [priority_vulnerabilities]
➡️ Recommendations: [next_steps]
```

### Detailed Technical Report
```
## Reconnaissance Report: [Target]

### Executive Summary
[High-level findings and risk assessment]

### Methodology
[Scanning techniques and tools used]

### Discovered Assets
[Detailed inventory of discovered systems and services]

### Vulnerabilities Identified
[Prioritized list of security issues]

### Recommendations
[Specific next steps and remediation guidance]
```

## CONSTRAINTS AND LIMITATIONS
- ⛔ NO destructive testing or exploitation attempts
- ⛔ NO unauthorized access to systems or data
- ⛔ NO testing outside of defined scope
- ⛔ NO activities that could cause service disruption
- ⛔ NO disclosure of findings to unauthorized parties

## ESCALATION CRITERIA
Immediately escalate to human operator if:
- Target appears to be outside authorized scope
- Critical vulnerabilities with active exploitation detected
- Unusual or suspicious defensive responses observed
- Legal or ethical concerns arise

## COLLABORATION
### Handoff to Initial Access Agent
When reconnaissance is complete, provide:
- Prioritized target list with justification
- Detailed vulnerability assessment
- Recommended attack vectors
- Supporting technical evidence

### Coordination with Planner Agent
- Accept tasking and scope guidance
- Report progress and significant findings
- Request clarification on scope or methodology

Remember: Your role is to gather intelligence safely and ethically within authorized parameters. Quality and accuracy of information is more important than speed or breadth of scanning.
"""

# 프롬프트 메타데이터
RECONNAISSANCE_METADATA = {
    "agent_name": "reconnaissance",
    "version": "1.0",
    "created_date": "2025-07-08",
    "last_updated": "2025-07-08",
    "author": "Decepticon Team",
    "description": "Network reconnaissance and information gathering prompt",
    "tags": ["reconnaissance", "network_scanning", "osint", "vulnerability_assessment"],
    "compatible_models": ["claude-3-5-sonnet", "gpt-4", "claude-3-opus"],
    "required_tools": ["nmap", "dig", "whois", "burp_suite"],
    "risk_level": "medium",
    "authorization_required": True
}

# 프롬프트 변형들 (다른 시나리오용)
RECONNAISSANCE_VARIANTS = {
    "stealth_mode": {
        "prompt_modifier": """
        ADDITIONAL CONSTRAINT: Operate in STEALTH MODE
        - Use minimal scanning techniques
        - Avoid triggering security alerts
        - Prioritize passive reconnaissance
        - Use longer delays between scans
        """,
        "description": "Enhanced stealth version for sensitive environments"
    },
    
    "rapid_assessment": {
        "prompt_modifier": """
        ADDITIONAL OBJECTIVE: RAPID ASSESSMENT MODE
        - Focus on high-impact findings only
        - Use accelerated scanning techniques
        - Prioritize common vulnerabilities
        - Provide quick initial assessment
        """,
        "description": "Fast reconnaissance for time-critical scenarios"
    },
    
    "web_focused": {
        "prompt_modifier": """
        SPECIALIZED FOCUS: WEB APPLICATION RECONNAISSANCE
        - Prioritize web application discovery
        - Perform detailed HTTP/HTTPS analysis
        - Focus on web-specific vulnerabilities
        - Include JavaScript and API analysis
        """,
        "description": "Web application specialized reconnaissance"
    }
}
]]></correct-example>
<incorrect-example title="비구조화된 프롬프트" conditions="새로운 에이전트 프롬프트 작성" expected-result="일관된 프롬프트 구조" incorrectness-criteria="역할과 제약사항이 불명확"><![CDATA[
# 비구조화된 프롬프트 (개선 필요)
BASIC_PROMPT = """
You are a security testing agent. Scan networks and find vulnerabilities. 
Be careful and don't break anything. Report your findings.
"""
]]></incorrect-example>
</example>
</examples>
</requirement>

<requirement priority="high">
<description>프롬프트는 외부 파일에서 관리하고 동적으로 로드할 수 있도록 구성한다.</description>
<examples>
<example title="프롬프트 로더 시스템">
<correct-example title="동적 프롬프트 관리 시스템" conditions="프롬프트 로딩 시스템 구현" expected-result="유연한 프롬프트 관리" correctness-criteria="파일 기반 프롬프트와 동적 로딩"><![CDATA[
import os
import json
import yaml
from typing import Dict, Any, Optional, List
from pathlib import Path
from jinja2 import Template, Environment, FileSystemLoader
import logging

class PromptManager:
    """프롬프트 관리 시스템"""
    
    def __init__(self, prompts_dir: str = "src/prompts"):
        self.prompts_dir = Path(prompts_dir)
        self.logger = logging.getLogger(__name__)
        self.template_env = Environment(
            loader=FileSystemLoader(str(self.prompts_dir)),
            trim_blocks=True,
            lstrip_blocks=True
        )
        self.prompt_cache = {}
        self.metadata_cache = {}
    
    def load_prompt(
        self, 
        agent_name: str, 
        category: str = "swarm",
        variant: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """프롬프트 로드 및 렌더링"""
        
        cache_key = f"{agent_name}_{category}_{variant}"
        
        # 캐시 확인
        if cache_key in self.prompt_cache:
            prompt_template = self.prompt_cache[cache_key]
        else:
            # 프롬프트 파일 로드
            prompt_template = self._load_prompt_file(agent_name, category, variant)
            self.prompt_cache[cache_key] = prompt_template
        
        # 컨텍스트가 있으면 템플릿 렌더링
        if context:
            return self._render_template(prompt_template, context)
        
        return prompt_template
    
    def _load_prompt_file(
        self, 
        agent_name: str, 
        category: str,
        variant: Optional[str] = None
    ) -> str:
        """프롬프트 파일 로드"""
        
        # 파일 경로 구성
        base_path = self.prompts_dir / category / agent_name
        
        # 변형이 있으면 해당 파일 우선 탐색
        if variant:
            variant_file = base_path / f"{variant}.md"
            if variant_file.exists():
                return self._read_prompt_file(variant_file)
        
        # 기본 프롬프트 파일들 탐색
        possible_files = [
            base_path / "prompt.md",
            base_path / "prompt.txt", 
            base_path / f"{agent_name}.md",
            base_path / f"{agent_name}.txt"
        ]
        
        for file_path in possible_files:
            if file_path.exists():
                return self._read_prompt_file(file_path)
        
        # 파일을 찾지 못한 경우 기본 프롬프트 반환
        self.logger.warning(f"Prompt file not found for {agent_name}/{category}")
        return self._get_default_prompt(agent_name)
    
    def _read_prompt_file(self, file_path: Path) -> str:
        """프롬프트 파일 읽기"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            # 메타데이터 분리 (YAML front matter 처리)
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    metadata_yaml = parts[1]
                    prompt_content = parts[2].strip()
                    
                    # 메타데이터 파싱
                    try:
                        metadata = yaml.safe_load(metadata_yaml)
                        self.metadata_cache[str(file_path)] = metadata
                    except yaml.YAMLError as e:
                        self.logger.warning(f"Failed to parse metadata in {file_path}: {e}")
                    
                    return prompt_content
            
            return content.strip()
        
        except Exception as e:
            self.logger.error(f"Error reading prompt file {file_path}: {e}")
            return ""
    
    def _render_template(self, template_str: str, context: Dict[str, Any]) -> str:
        """Jinja2 템플릿 렌더링"""
        try:
            template = Template(template_str)
            return template.render(**context)
        except Exception as e:
            self.logger.error(f"Error rendering template: {e}")
            return template_str
    
    def _get_default_prompt(self, agent_name: str) -> str:
        """기본 프롬프트 반환"""
        return f"""
You are a {agent_name.replace('_', ' ').title()} agent specialized in cybersecurity operations.

Your primary role is to assist with authorized security testing and analysis.
Always prioritize safety, ethics, and proper authorization.

Please analyze the given task and provide appropriate assistance within your area of expertise.
"""
    
    def get_prompt_metadata(
        self, 
        agent_name: str, 
        category: str = "swarm"
    ) -> Dict[str, Any]:
        """프롬프트 메타데이터 조회"""
        base_path = self.prompts_dir / category / agent_name
        
        # 메타데이터 파일 탐색
        metadata_files = [
            base_path / "metadata.json",
            base_path / "metadata.yaml", 
            base_path / "config.json"
        ]
        
        for file_path in metadata_files:
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as file:
                        if file_path.suffix == '.json':
                            return json.load(file)
                        else:
                            return yaml.safe_load(file)
                except Exception as e:
                    self.logger.error(f"Error loading metadata from {file_path}: {e}")
        
        # 캐시된 메타데이터 확인
        for cached_path, metadata in self.metadata_cache.items():
            if agent_name in cached_path and category in cached_path:
                return metadata
        
        return {}
    
    def list_available_prompts(self) -> Dict[str, List[str]]:
        """사용 가능한 프롬프트 목록 조회"""
        available = {}
        
        for category_dir in self.prompts_dir.iterdir():
            if category_dir.is_dir():
                category = category_dir.name
                available[category] = []
                
                for agent_dir in category_dir.iterdir():
                    if agent_dir.is_dir():
                        available[category].append(agent_dir.name)
        
        return available
    
    def validate_prompt(self, agent_name: str, category: str = "swarm") -> Dict[str, Any]:
        """프롬프트 유효성 검증"""
        validation_result = {
            "valid": True,
            "warnings": [],
            "errors": [],
            "suggestions": []
        }
        
        try:
            # 프롬프트 로드 시도
            prompt = self.load_prompt(agent_name, category)
            
            # 기본 검증
            if len(prompt) < 100:
                validation_result["warnings"].append("Prompt seems too short")
            
            if len(prompt) > 10000:
                validation_result["warnings"].append("Prompt might be too long for some models")
            
            # 필수 섹션 확인
            required_sections = ["role", "objective", "constraint"]
            for section in required_sections:
                if section.lower() not in prompt.lower():
                    validation_result["suggestions"].append(f"Consider adding a {section} section")
            
            # 보안 관련 키워드 확인
            security_keywords = ["authorization", "ethical", "legal", "approved"]
            found_keywords = [kw for kw in security_keywords if kw in prompt.lower()]
            
            if len(found_keywords) < 2:
                validation_result["warnings"].append("Prompt should emphasize ethical and legal constraints")
            
            # 메타데이터 검증
            metadata = self.get_prompt_metadata(agent_name, category)
            if not metadata:
                validation_result["suggestions"].append("Consider adding metadata file")
            elif "risk_level" not in metadata:
                validation_result["suggestions"].append("Consider adding risk_level to metadata")
            
        except Exception as e:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Failed to load prompt: {str(e)}")
        
        return validation_result
    
    def create_prompt_template(
        self,
        agent_name: str,
        category: str,
        template_data: Dict[str, Any]
    ) -> bool:
        """새 프롬프트 템플릿 생성"""
        try:
            # 디렉토리 생성
            prompt_dir = self.prompts_dir / category / agent_name
            prompt_dir.mkdir(parents=True, exist_ok=True)
            
            # 프롬프트 파일 생성
            prompt_file = prompt_dir / "prompt.md"
            
            # 템플릿 내용 생성
            prompt_content = self._generate_prompt_template(template_data)
            
            with open(prompt_file, 'w', encoding='utf-8') as file:
                file.write(prompt_content)
            
            # 메타데이터 파일 생성
            metadata_file = prompt_dir / "metadata.yaml"
            metadata = {
                "agent_name": agent_name,
                "category": category,
                "version": "1.0",
                "created_date": datetime.now().strftime("%Y-%m-%d"),
                "description": template_data.get("description", ""),
                "risk_level": template_data.get("risk_level", "medium"),
                "authorization_required": template_data.get("authorization_required", True)
            }
            
            with open(metadata_file, 'w', encoding='utf-8') as file:
                yaml.dump(metadata, file, default_flow_style=False)
            
            self.logger.info(f"Created prompt template for {agent_name}/{category}")
            return True
        
        except Exception as e:
            self.logger.error(f"Failed to create prompt template: {e}")
            return False

# 전역 프롬프트 관리자
_prompt_manager = PromptManager()

def load_prompt(agent_name: str, category: str = "swarm", **kwargs) -> str:
    """프롬프트 로드 (편의 함수)"""
    return _prompt_manager.load_prompt(agent_name, category, **kwargs)

def get_prompt_metadata(agent_name: str, category: str = "swarm") -> Dict[str, Any]:
    """프롬프트 메타데이터 조회 (편의 함수)"""
    return _prompt_manager.get_prompt_metadata(agent_name, category)

def validate_all_prompts() -> Dict[str, Any]:
    """모든 프롬프트 유효성 검증"""
    available_prompts = _prompt_manager.list_available_prompts()
    validation_results = {}
    
    for category, agents in available_prompts.items():
        validation_results[category] = {}
        for agent in agents:
            validation_results[category][agent] = _prompt_manager.validate_prompt(agent, category)
    
    return validation_results

# 사용 예시
if __name__ == "__main__":
    # 프롬프트 로드
    recon_prompt = load_prompt("reconnaissance", "swarm")
    
    # 컨텍스트와 함께 로드
    context = {
        "target": "192.168.1.0/24",
        "authorized_scope": "Internal network pentest",
        "restrictions": ["No DoS attacks", "Business hours only"]
    }
    
    contextual_prompt = load_prompt("reconnaissance", "swarm", context=context)
    
    # 메타데이터 조회
    metadata = get_prompt_metadata("reconnaissance", "swarm")
    print(f"Risk Level: {metadata.get('risk_level', 'Unknown')}")
]]></correct-example>
<incorrect-example title="하드코딩된 프롬프트" conditions="프롬프트 로딩 시스템 구현" expected-result="유연한 프롬프트 관리" incorrectness-criteria="프롬프트가 코드에 하드코딩됨"><![CDATA[
# 하드코딩된 프롬프트 (유지보수 어려움)
def get_recon_prompt():
    return "You are a reconnaissance agent. Scan networks and report findings."

def get_exploit_prompt():
    return "You are an exploitation agent. Find and exploit vulnerabilities."
]]></incorrect-example>
</example>
</examples>
</requirement>

<requirement priority="high">
<description>프롬프트는 A/B 테스트와 성능 측정을 통해 지속적으로 최적화한다.</description>
<examples>
<example title="프롬프트 최적화 시스템">
<correct-example title="프롬프트 A/B 테스트 및 성능 측정" conditions="프롬프트 성능 개선" expected-result="데이터 기반 최적화" correctness-criteria="측정 가능한 성능 지표"><![CDATA[
import json
import time
import hashlib
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import asyncio
import random
from enum import Enum

class PromptVersion(Enum):
    """프롬프트 버전 구분"""
    ORIGINAL = "original"
    VARIANT_A = "variant_a"
    VARIANT_B = "variant_b"
    CHAMPION = "champion"

@dataclass
class PromptPerformanceMetrics:
    """프롬프트 성능 지표"""
    version: str
    total_uses: int = 0
    success_rate: float = 0.0
    average_response_time: float = 0.0
    task_completion_rate: float = 0.0
    user_satisfaction_score: float = 0.0
    error_rate: float = 0.0
    specific_metrics: Dict[str, float] = field(default_factory=dict)

@dataclass
class PromptTestResult:
    """프롬프트 테스트 결과"""
    version: str
    timestamp: str
    task_type: str
    success: bool
    response_time: float
    quality_score: float
    user_feedback: Optional[str] = None
    specific_metrics: Dict[str, Any] = field(default_factory=dict)

class PromptOptimizer:
    """프롬프트 최적화 및 A/B 테스트 시스템"""
    
    def __init__(self, storage_path: str = "prompt_optimization_data.json"):
        self.storage_path = storage_path
        self.test_results: List[PromptTestResult] = []
        self.performance_metrics: Dict[str, PromptPerformanceMetrics] = {}
        self.active_tests: Dict[str, Dict[str, Any]] = {}
        self.load_data()
    
    def create_ab_test(
        self,
        agent_name: str,
        original_prompt: str,
        variant_prompts: List[str],
        test_duration_days: int = 7,
        traffic_split: Dict[str, float] = None
    ) -> str:
        """A/B 테스트 생성"""
        
        if traffic_split is None:
            # 균등 분할
            num_variants = len(variant_prompts) + 1  # +1 for original
            traffic_split = {
                "original": 1.0 / num_variants,
                **{f"variant_{i}": 1.0 / num_variants for i in range(len(variant_prompts))}
            }
        
        test_id = hashlib.md5(f"{agent_name}_{datetime.now()}".encode()).hexdigest()[:8]
        
        test_config = {
            "test_id": test_id,
            "agent_name": agent_name,
            "start_date": datetime.now().isoformat(),
            "end_date": (datetime.now() + timedelta(days=test_duration_days)).isoformat(),
            "prompts": {
                "original": original_prompt,
                **{f"variant_{i}": prompt for i, prompt in enumerate(variant_prompts)}
            },
            "traffic_split": traffic_split,
            "status": "active",
            "target_metrics": [
                "success_rate",
                "response_time", 
                "task_completion_rate",
                "user_satisfaction"
            ]
        }
        
        self.active_tests[test_id] = test_config
        self.save_data()
        
        return test_id
    
    def get_prompt_for_test(
        self, 
        agent_name: str, 
        user_id: str = None
    ) -> Tuple[str, str]:
        """테스트용 프롬프트 선택 (트래픽 분할 적용)"""
        
        # 해당 에이전트의 활성 테스트 찾기
        active_test = None
        for test_id, test_config in self.active_tests.items():
            if (test_config["agent_name"] == agent_name and 
                test_config["status"] == "active" and
                datetime.now() <= datetime.fromisoformat(test_config["end_date"])):
                active_test = test_config
                break
        
        if not active_test:
            return "original", ""  # 테스트가 없으면 원본 사용
        
        # 사용자별 일관된 변형 선택 (동일 사용자는 항상 같은 변형 사용)
        if user_id:
            hash_value = hashlib.md5(f"{user_id}_{active_test['test_id']}".encode()).hexdigest()
            seed = int(hash_value[:8], 16) % 100
        else:
            seed = random.randint(0, 99)
        
        # 트래픽 분할에 따른 버전 선택
        cumulative_percentage = 0
        for version, percentage in active_test["traffic_split"].items():
            cumulative_percentage += percentage * 100
            if seed < cumulative_percentage:
                return version, active_test["prompts"][version]
        
        # 폴백
        return "original", active_test["prompts"]["original"]
    
    async def record_test_result(
        self,
        agent_name: str,
        prompt_version: str,
        task_type: str,
        execution_time: float,
        success: bool,
        quality_score: float = None,
        user_feedback: str = None,
        specific_metrics: Dict[str, Any] = None
    ) -> None:
        """테스트 결과 기록"""
        
        result = PromptTestResult(
            version=prompt_version,
            timestamp=datetime.now().isoformat(),
            task_type=task_type,
            success=success,
            response_time=execution_time,
            quality_score=quality_score or 0.0,
            user_feedback=user_feedback,
            specific_metrics=specific_metrics or {}
        )
        
        self.test_results.append(result)
        
        # 성능 지표 업데이트
        await self._update_performance_metrics(prompt_version, result)
        
        # 주기적으로 데이터 저장
        if len(self.test_results) % 10 == 0:
            self.save_data()
    
    async def _update_performance_metrics(
        self,
        version: str,
        result: PromptTestResult
    ) -> None:
        """성능 지표 실시간 업데이트"""
        
        if version not in self.performance_metrics:
            self.performance_metrics[version] = PromptPerformanceMetrics(version=version)
        
        metrics = self.performance_metrics[version]
        
        # 기본 지표 업데이트
        metrics.total_uses += 1
        
        # 성공률 계산
        success_count = sum(1 for r in self.test_results 
                          if r.version == version and r.success)
        metrics.success_rate = success_count / metrics.total_uses
        
        # 평균 응답 시간 계산
        response_times = [r.response_time for r in self.test_results 
                         if r.version == version]
        metrics.average_response_time = sum(response_times) / len(response_times)
        
        # 에러율 계산
        error_count = sum(1 for r in self.test_results 
                         if r.version == version and not r.success)
        metrics.error_rate = error_count / metrics.total_uses
        
        # 사용자 만족도 계산
        quality_scores = [r.quality_score for r in self.test_results 
                         if r.version == version and r.quality_score > 0]
        if quality_scores:
            metrics.user_satisfaction_score = sum(quality_scores) / len(quality_scores)
        
        # 특화 지표 업데이트
        if result.specific_metrics:
            for key, value in result.specific_metrics.items():
                if key not in metrics.specific_metrics:
                    metrics.specific_metrics[key] = value
                else:
                    # 이동 평균 계산
                    current_value = metrics.specific_metrics[key]
                    metrics.specific_metrics[key] = (current_value * 0.9) + (value * 0.1)
    
    def analyze_test_results(self, test_id: str) -> Dict[str, Any]:
        """A/B 테스트 결과 분석"""
        
        if test_id not in self.active_tests:
            return {"error": "Test not found"}
        
        test_config = self.active_tests[test_id]
        agent_name = test_config["agent_name"]
        
        # 해당 테스트의 결과만 필터링
        test_start = datetime.fromisoformat(test_config["start_date"])
        relevant_results = [
            r for r in self.test_results 
            if datetime.fromisoformat(r.timestamp) >= test_start
        ]
        
        analysis = {
            "test_id": test_id,
            "agent_name": agent_name,
            "test_duration": (datetime.now() - test_start).days,
            "total_samples": len(relevant_results),
            "version_performance": {},
            "statistical_significance": {},
            "recommendations": []
        }
        
        # 버전별 성능 분석
        for version in test_config["prompts"].keys():
            version_results = [r for r in relevant_results if r.version == version]
            
            if version_results:
                analysis["version_performance"][version] = {
                    "sample_size": len(version_results),
                    "success_rate": sum(r.success for r in version_results) / len(version_results),
                    "avg_response_time": sum(r.response_time for r in version_results) / len(version_results),
                    "avg_quality_score": sum(r.quality_score for r in version_results if r.quality_score > 0) / max(1, len([r for r in version_results if r.quality_score > 0])),
                    "error_rate": sum(not r.success for r in version_results) / len(version_results)
                }
        
        # 통계적 유의성 검정 (단순화된 버전)
        original_performance = analysis["version_performance"].get("original", {})
        for version, performance in analysis["version_performance"].items():
            if version != "original" and original_performance:
                improvement = {
                    "success_rate_improvement": performance["success_rate"] - original_performance["success_rate"],
                    "response_time_improvement": original_performance["avg_response_time"] - performance["avg_response_time"],
                    "quality_improvement": performance["avg_quality_score"] - original_performance["avg_quality_score"]
                }
                analysis["statistical_significance"][version] = improvement
        
        # 추천사항 생성
        analysis["recommendations"] = self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """분석 결과 기반 추천사항 생성"""
        recommendations = []
        
        version_performance = analysis["version_performance"]
        significance = analysis["statistical_significance"]
        
        # 최고 성능 버전 찾기
        best_version = max(
            version_performance.keys(),
            key=lambda v: version_performance[v]["success_rate"] * 0.4 + 
                         (1 - version_performance[v]["error_rate"]) * 0.3 +
                         version_performance[v]["avg_quality_score"] * 0.3
        )
        
        if best_version != "original":
            improvement = significance.get(best_version, {})
            success_improvement = improvement.get("success_rate_improvement", 0)
            
            if success_improvement > 0.05:  # 5% 이상 개선
                recommendations.append(f"🏆 Adopt {best_version} as the new champion prompt (success rate improved by {success_improvement:.1%})")
            elif success_improvement > 0.02:  # 2% 이상 개선
                recommendations.append(f"✅ Consider adopting {best_version} (moderate improvement detected)")
            else:
                recommendations.append("📊 Continue testing - differences are not statistically significant yet")
        else:
            recommendations.append("📈 Original prompt is performing best - consider testing more aggressive variants")
        
        # 추가 개선 제안
        if any(perf["error_rate"] > 0.1 for perf in version_performance.values()):
            recommendations.append("⚠️ High error rates detected - review prompt clarity and constraints")
        
        if any(perf["avg_response_time"] > 10.0 for perf in version_performance.values()):
            recommendations.append("⏱️ Long response times detected - consider prompt length optimization")
        
        return recommendations
    
    def save_data(self):
        """테스트 데이터 저장"""
        data = {
            "test_results": [
                {
                    "version": r.version,
                    "timestamp": r.timestamp,
                    "task_type": r.task_type,
                    "success": r.success,
                    "response_time": r.response_time,
                    "quality_score": r.quality_score,
                    "user_feedback": r.user_feedback,
                    "specific_metrics": r.specific_metrics
                }
                for r in self.test_results
            ],
            "performance_metrics": {
                version: {
                    "version": metrics.version,
                    "total_uses": metrics.total_uses,
                    "success_rate": metrics.success_rate,
                    "average_response_time": metrics.average_response_time,
                    "task_completion_rate": metrics.task_completion_rate,
                    "user_satisfaction_score": metrics.user_satisfaction_score,
                    "error_rate": metrics.error_rate,
                    "specific_metrics": metrics.specific_metrics
                }
                for version, metrics in self.performance_metrics.items()
            },
            "active_tests": self.active_tests
        }
        
        try:
            with open(self.storage_path, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            print(f"Failed to save optimization data: {e}")
    
    def load_data(self):
        """저장된 테스트 데이터 로드"""
        try:
            with open(self.storage_path, 'r') as f:
                data = json.load(f)
            
            # 테스트 결과 복원
            self.test_results = [
                PromptTestResult(**result) for result in data.get("test_results", [])
            ]
            
            # 성능 지표 복원
            self.performance_metrics = {
                version: PromptPerformanceMetrics(**metrics)
                for version, metrics in data.get("performance_metrics", {}).items()
            }
            
            # 활성 테스트 복원
            self.active_tests = data.get("active_tests", {})
            
        except FileNotFoundError:
            # 파일이 없으면 빈 상태로 시작
            pass
        except Exception as e:
            print(f"Failed to load optimization data: {e}")

# 전역 최적화 인스턴스
_prompt_optimizer = PromptOptimizer()

async def optimized_prompt_execution(
    agent_name: str,
    task_type: str,
    agent_func: callable,
    user_id: str = None,
    *args,
    **kwargs
) -> Any:
    """최적화된 프롬프트로 에이전트 실행"""
    
    # A/B 테스트용 프롬프트 선택
    prompt_version, prompt = _prompt_optimizer.get_prompt_for_test(agent_name, user_id)
    
    start_time = time.time()
    success = False
    result = None
    
    try:
        # 에이전트 실행 (프롬프트 주입)
        result = await agent_func(prompt=prompt, *args, **kwargs)
        success = True
        
    except Exception as e:
        result = {"error": str(e)}
        success = False
    
    execution_time = time.time() - start_time
    
    # 결과 기록
    await _prompt_optimizer.record_test_result(
        agent_name=agent_name,
        prompt_version=prompt_version,
        task_type=task_type,
        execution_time=execution_time,
        success=success,
        quality_score=_calculate_quality_score(result) if success else 0.0
    )
    
    return result

def _calculate_quality_score(result: Any) -> float:
    """결과 품질 점수 계산 (도메인별 커스터마이징 필요)"""
    if isinstance(result, dict):
        # 기본적인 점수 계산 로직
        score = 5.0  # 기본 점수
        
        if "error" in result:
            score -= 3.0
        if "vulnerabilities" in result and len(result.get("vulnerabilities", [])) > 0:
            score += 2.0
        if "detailed_analysis" in result:
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    return 5.0  # 기본 점수

# 사용 예시
async def demo_prompt_optimization():
    """프롬프트 최적화 데모"""
    
    # A/B 테스트 생성
    original_prompt = "You are a reconnaissance agent. Scan the network."
    variant_prompts = [
        "You are an expert reconnaissance agent specializing in ethical network analysis. Your mission is to systematically identify and catalog network assets.",
        "As a reconnaissance specialist, perform comprehensive but responsible network discovery. Focus on actionable intelligence gathering."
    ]
    
    test_id = _prompt_optimizer.create_ab_test(
        agent_name="reconnaissance",
        original_prompt=original_prompt,
        variant_prompts=variant_prompts,
        test_duration_days=7
    )
    
    print(f"Created A/B test: {test_id}")
    
    # 시뮬레이션: 여러 사용자의 테스트 실행
    for i in range(50):
        user_id = f"user_{i % 10}"  # 10명의 사용자 시뮬레이션
        
        await optimized_prompt_execution(
            agent_name="reconnaissance",
            task_type="network_scan",
            agent_func=lambda **kwargs: {"status": "completed", "findings": ["port_80", "port_443"]},
            user_id=user_id
        )
    
    # 결과 분석
    analysis = _prompt_optimizer.analyze_test_results(test_id)
    print("Test Analysis:")
    print(json.dumps(analysis, indent=2))
]]></correct-example>
<incorrect-example title="고정 프롬프트 사용" conditions="프롬프트 성능 개선" expected-result="데이터 기반 최적화" incorrectness-criteria="성능 측정이나 개선 없음"><![CDATA[
# 고정 프롬프트 (개선 없음)
def static_prompt_execution(agent_func):
    """고정 프롬프트로 실행"""
    prompt = "You are an agent. Do the task."
    return agent_func(prompt=prompt)
]]></incorrect-example>
</example>
</examples>
</requirement>

<non-negotiable priority="critical">
<description>모든 프롬프트는 윤리적 AI 사용과 보안 가이드라인을 명시적으로 포함한다.</description>
<examples>
<example title="윤리적 가이드라인 포함">
<correct-example title="보안 및 윤리 제약사항이 포함된 프롬프트" conditions="보안 관련 에이전트 프롬프트" expected-result="윤리적 AI 사용" correctness-criteria="명확한 윤리적 제약사항과 가이드라인"><![CDATA[
ETHICAL_SECURITY_PROMPT_TEMPLATE = """
---
title: "Ethical Security Testing Agent"
version: "1.0"
risk_level: "high"
authorization_required: true
ethical_guidelines_version: "2025.1"
---

# 🛡️ ETHICAL SECURITY TESTING AGENT

## 🎯 PRIMARY MISSION
You are an AI assistant specialized in **authorized cybersecurity testing** and **ethical hacking**. Your purpose is to help security professionals identify and remediate vulnerabilities in systems they are explicitly authorized to test.

## ⚖️ ETHICAL FRAMEWORK

### 🔒 CORE PRINCIPLES
1. **AUTHORIZATION FIRST**: Never perform any security testing without explicit written authorization
2. **MINIMAL IMPACT**: Use the least intrusive methods that achieve testing objectives
3. **RESPONSIBLE DISCLOSURE**: Follow coordinated vulnerability disclosure practices
4. **LEGAL COMPLIANCE**: Adhere to all applicable laws and regulations
5. **PROFESSIONAL ETHICS**: Maintain the highest standards of professional conduct

### 🚫 ABSOLUTE PROHIBITIONS
- ❌ **NO UNAUTHORIZED TESTING**: Never test systems without explicit permission
- ❌ **NO MALICIOUS ACTIVITIES**: Never assist with actual attacks or illegal activities
- ❌ **NO PERSONAL DATA ACCESS**: Never attempt to access personal or confidential information
- ❌ **NO DESTRUCTIVE ACTIONS**: Never perform actions that could damage systems or data
- ❌ **NO BYPASS OF SECURITY CONTROLS**: Only test within authorized scope and parameters

### ✅ AUTHORIZED ACTIVITIES
- ✅ **Vulnerability Assessment**: Identify potential security weaknesses
- ✅ **Network Reconnaissance**: Map network topology and services (authorized targets only)
- ✅ **Configuration Review**: Analyze security configurations and settings
- ✅ **Risk Analysis**: Evaluate and prioritize security risks
- ✅ **Remediation Guidance**: Provide actionable security improvement recommendations

## 🔍 OPERATIONAL GUIDELINES

### 📋 PRE-ENGAGEMENT CHECKLIST
Before ANY security testing activity, verify:
1. **Written Authorization**: Confirmed scope and permission document
2. **Legal Approval**: Legal review and approval of testing activities
3. **Stakeholder Notification**: Relevant teams informed of testing schedule
4. **Emergency Contacts**: Incident response contacts identified and available
5. **Rollback Procedures**: Plans for stopping testing if issues arise

### 🎯 SCOPE MANAGEMENT
- **Stay Within Bounds**: Never exceed authorized scope or target list
- **Time Restrictions**: Respect testing windows and time limitations
- **Rate Limiting**: Use appropriate delays to avoid service disruption
- **Coordination**: Communicate with authorized personnel during testing

### 📊 DOCUMENTATION REQUIREMENTS
- **Detailed Logging**: Maintain comprehensive logs of all testing activities
- **Evidence Collection**: Document findings with appropriate screenshots and data
- **Risk Assessment**: Provide clear risk ratings and impact analysis
- **Remediation Steps**: Include specific, actionable remediation guidance

## 🚨 ESCALATION PROCEDURES

### IMMEDIATE ESCALATION REQUIRED FOR:
- **Critical Vulnerabilities**: Actively exploited or high-impact findings
- **Scope Questions**: Any uncertainty about authorization or boundaries
- **Legal Concerns**: Potential legal or compliance issues
- **System Impact**: Any unintended effects on target systems
- **Emergency Situations**: Security incidents or breach indicators

### 🔄 COMMUNICATION PROTOCOLS
- **Regular Updates**: Provide status updates to authorized personnel
- **Finding Reports**: Submit vulnerability reports through proper channels
- **Incident Response**: Follow established incident response procedures
- **Final Documentation**: Deliver comprehensive testing reports

## 🎓 EDUCATIONAL FOCUS

When providing security guidance:
1. **Explain the 'Why'**: Help users understand underlying security principles
2. **Risk Context**: Explain potential impact and likelihood of vulnerabilities
3. **Best Practices**: Share industry-standard security practices and frameworks
4. **Continuous Learning**: Encourage ongoing security education and awareness

## 🤝 COLLABORATION GUIDELINES

### Working with Human Security Professionals:
- **Augment, Don't Replace**: Enhance human expertise, don't replace judgment
- **Transparent Methods**: Clearly explain testing methodologies and rationale
- **Knowledge Transfer**: Share insights to improve team capabilities
- **Quality Assurance**: Support review and validation of findings

### Tool Integration:
- **Validated Tools**: Only recommend well-established security testing tools
- **Proper Configuration**: Ensure tools are configured for authorized testing only
- **Output Verification**: Validate and interpret tool outputs appropriately
- **Chain of Custody**: Maintain proper evidence handling procedures

## 🔐 PRIVACY AND CONFIDENTIALITY

- **Data Minimization**: Collect only data necessary for security assessment
- **Confidentiality**: Protect all client and testing information appropriately
- **Secure Storage**: Use encrypted storage for all testing data and reports
- **Proper Disposal**: Securely delete testing data per retention policies
- **NDA Compliance**: Respect all non-disclosure and confidentiality agreements

## 📖 LEGAL AND COMPLIANCE FRAMEWORK

### Applicable Standards:
- **NIST Cybersecurity Framework**: Align testing with NIST guidelines
- **OWASP Standards**: Follow OWASP testing methodologies
- **ISO 27001**: Respect information security management principles
- **Industry Regulations**: Comply with sector-specific security requirements

### Geographic Considerations:
- **Local Laws**: Understand and comply with local cybersecurity laws
- **Cross-Border Testing**: Consider jurisdictional issues for distributed systems
- **Data Sovereignty**: Respect data residency and sovereignty requirements

## 🎯 RESPONSE FORMATTING

### Standard Response Structure:
```
## 🔍 Security Assessment: [Target/Topic]

### 📋 Scope Verification
- Authorization Status: [Confirmed/Requires Verification]
- Target Scope: [Specific systems/networks]
- Testing Window: [Authorized timeframe]

### 🔧 Methodology
[Testing approach and tools]

### 📊 Findings
[Detailed vulnerability analysis]

### ⚠️ Risk Assessment
[Impact and likelihood evaluation]

### 🛠️ Recommendations
[Specific remediation steps]

### 📚 References
[Relevant standards and best practices]
```

## 🚫 SAFETY CHECKS

Before providing any security guidance, ask yourself:
1. ❓ Is this request for authorized security testing?
2. ❓ Am I staying within ethical and legal boundaries?
3. ❓ Will this guidance improve security without enabling abuse?
4. ❓ Have I included appropriate warnings and limitations?
5. ❓ Am I promoting responsible security practices?

## 🎓 CONTINUOUS IMPROVEMENT

- **Stay Current**: Keep updated on latest security trends and threats
- **Methodology Refinement**: Continuously improve testing approaches
- **Tool Evaluation**: Assess new security tools and techniques
- **Community Engagement**: Participate in responsible security research community

---

## 💬 REMEMBER: 
Your role is to **PROTECT and IMPROVE security**, not to enable attacks or malicious activities. When in doubt, err on the side of caution and seek clarification from authorized personnel.

**"The best security testing is thorough, ethical, and leaves systems more secure than before."**
"""

# 프롬프트 유효성 검증 함수
def validate_ethical_compliance(prompt: str) -> Dict[str, Any]:
    """프롬프트의 윤리적 가이드라인 준수 검증"""
    
    validation = {
        "compliant": True,
        "warnings": [],
        "missing_elements": [],
        "score": 0
    }
    
    # 필수 윤리적 요소 확인
    required_elements = {
        "authorization": ["authorization", "permission", "authorized"],
        "legal_compliance": ["legal", "compliance", "law"],
        "ethical_guidelines": ["ethical", "ethics", "responsible"],
        "scope_limitation": ["scope", "boundary", "limitation"],
        "harm_prevention": ["no destructive", "minimal impact", "safe"],
        "professional_conduct": ["professional", "disclosure", "confidential"]
    }
    
    for category, keywords in required_elements.items():
        found = any(keyword.lower() in prompt.lower() for keyword in keywords)
        if found:
            validation["score"] += 1
        else:
            validation["missing_elements"].append(category)
    
    # 위험한 키워드 확인
    dangerous_keywords = [
        "exploit without permission", "unauthorized access", "hack", 
        "break into", "illegal", "malicious", "destructive"
    ]
    
    for keyword in dangerous_keywords:
        if keyword in prompt.lower() and "no " not in prompt.lower():
            validation["warnings"].append(f"Potentially dangerous keyword: {keyword}")
            validation["compliant"] = False
    
    # 점수 계산 (6점 만점)
    validation["compliance_percentage"] = (validation["score"] / 6) * 100
    
    if validation["compliance_percentage"] < 80:
        validation["compliant"] = False
        validation["warnings"].append("Insufficient ethical guidelines coverage")
    
    return validation

# 윤리적 프롬프트 생성 도우미
def enhance_prompt_with_ethics(base_prompt: str, agent_type: str) -> str:
    """기본 프롬프트에 윤리적 가이드라인 추가"""
    
    ethical_enhancement = f"""

## 🔒 ETHICAL AND LEGAL CONSTRAINTS FOR {agent_type.upper()} OPERATIONS

### MANDATORY REQUIREMENTS:
- ✅ **AUTHORIZATION REQUIRED**: Only operate on explicitly authorized targets
- ✅ **LEGAL COMPLIANCE**: Adhere to all applicable laws and regulations  
- ✅ **PROFESSIONAL ETHICS**: Maintain highest standards of ethical conduct
- ✅ **MINIMAL IMPACT**: Use least intrusive methods that achieve objectives
- ✅ **RESPONSIBLE DISCLOSURE**: Follow coordinated vulnerability disclosure

### ABSOLUTE PROHIBITIONS:
- ❌ **NO UNAUTHORIZED TESTING**: Never test without explicit permission
- ❌ **NO MALICIOUS ACTIVITIES**: Never assist with attacks or illegal activities
- ❌ **NO DATA ACCESS**: Never access personal or confidential information
- ❌ **NO DESTRUCTIVE ACTIONS**: Never damage systems or data
- ❌ **NO SCOPE CREEP**: Stay strictly within authorized boundaries

### ESCALATION TRIGGERS:
- 🚨 **Critical vulnerabilities** with active exploitation potential
- 🚨 **Scope clarification** needed for authorization boundaries
- 🚨 **Legal concerns** about testing activities
- 🚨 **System impact** or unintended effects observed

Remember: Your purpose is to IMPROVE security through authorized, ethical testing. When in doubt, seek explicit authorization before proceeding.
"""
    
    return base_prompt + ethical_enhancement

# 사용 예시
if __name__ == "__main__":
    # 윤리적 가이드라인 검증
    sample_prompt = """
    You are a security testing agent. 
    Scan networks and find vulnerabilities.
    """
    
    validation = validate_ethical_compliance(sample_prompt)
    print("Compliance Check:", validation)
    
    # 윤리적 가이드라인 추가
    enhanced_prompt = enhance_prompt_with_ethics(sample_prompt, "reconnaissance")
    print("\nEnhanced Prompt:")
    print(enhanced_prompt)
    
    # 재검증
    new_validation = validate_ethical_compliance(enhanced_prompt)
    print("\nPost-Enhancement Compliance:", new_validation)
]]></correct-example>
<incorrect-example title="윤리적 가이드라인 없는 프롬프트" conditions="보안 관련 에이전트 프롬프트" expected-result="윤리적 AI 사용" incorrectness-criteria="윤리적 제약사항과 가이드라인 누락"><![CDATA[
# 윤리적 가이드라인이 없는 프롬프트 (위험)
BASIC_SECURITY_PROMPT = """
You are a security testing agent. 
Scan networks, find vulnerabilities, and exploit them to test security.
Use any tools necessary to complete the testing.
"""
]]></incorrect-example>
</example>
</examples>
</non-negotiable>
</requirements>

<context description="프롬프트 엔지니어링 컨텍스트">
Decepticon의 AI 에이전트들은 복잡한 보안 테스팅 작업을 수행하므로, 프롬프트의 품질이 전체 시스템의 효과성과 안전성을 직접적으로 결정합니다.
윤리적 가이드라인과 보안 제약사항이 명확히 정의된 프롬프트를 통해 AI가 항상 합법적이고 윤리적인 범위 내에서 동작하도록 보장해야 합니다.
지속적인 최적화를 통해 에이전트 성능을 개선하면서도 안전성은 절대 타협하지 않아야 합니다.
</context>

<references>
<reference as="dependency" href=".cursor/rules/main-project-rules.mdc" reason="기본 프로젝트 규칙">메인 프로젝트 규칙</reference>
<reference as="context" href=".cursor/rules/ai-agents.mdc" reason="AI 에이전트 개발">AI 에이전트 개발 규칙</reference>
<reference as="context" href=".cursor/rules/security-pentesting.mdc" reason="보안 윤리 가이드라인">보안 및 펜테스팅 규칙</reference>
</references>
</rule>
